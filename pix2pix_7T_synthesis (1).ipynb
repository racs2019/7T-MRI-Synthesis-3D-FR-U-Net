{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9Zg5VhRDNHi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import glob\n",
        "import datetime\n",
        "import zipfile\n",
        "import random\n",
        "import requests\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torchio as tio\n",
        "import torch\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_nifti(file_path):\n",
        "    \"\"\"\n",
        "    Load a NIfTI file and normalize it to [0, 1].\n",
        "    \"\"\"\n",
        "    img = nib.load(file_path)\n",
        "    data = img.get_fdata()\n",
        "    data = data / 4095.0  # Normalization\n",
        "    return data.astype(np.float32)\n",
        "\n",
        "def extract_all_slices(volume, resize_to=(256, 256)):\n",
        "    \"\"\"\n",
        "    Extracts all 2D slices from axial, coronal, and sagittal planes.\n",
        "    Skips slices with only zero values. Resizes to (256, 256).\n",
        "    \"\"\"\n",
        "    slices = []\n",
        "\n",
        "    # Axial: slices along the Z-axis\n",
        "    for i in range(volume.shape[2]):\n",
        "        s = volume[:, :, i]\n",
        "        if np.max(s) != 0:\n",
        "            s = tf.image.resize(s[..., np.newaxis], resize_to).numpy()\n",
        "            slices.append(s)\n",
        "\n",
        "    # Coronal: slices along the Y-axis\n",
        "    for i in range(volume.shape[1]):\n",
        "        s = volume[:, i, :]\n",
        "        if np.max(s) != 0:\n",
        "            s = tf.image.resize(s[..., np.newaxis], resize_to).numpy()\n",
        "            slices.append(s)\n",
        "\n",
        "    # Sagittal: slices along the X-axis\n",
        "    for i in range(volume.shape[0]):\n",
        "        s = volume[i, :, :]\n",
        "        if np.max(s) != 0:\n",
        "            s = tf.image.resize(s[..., np.newaxis], resize_to).numpy()\n",
        "            slices.append(s)\n",
        "\n",
        "    return np.array(slices)\n",
        "\n",
        "def load_and_extract_slices(synthetic_files, ground_truth_files, resize_to=(256, 256)):\n",
        "    synthetic_slices = []\n",
        "    ground_truth_slices = []\n",
        "\n",
        "    for synthetic_path, gt_path in zip(synthetic_files, ground_truth_files):\n",
        "        print(f\"Processing: {synthetic_path} and {gt_path}\")\n",
        "        syn_vol = load_nifti(synthetic_path)\n",
        "        gt_vol = load_nifti(gt_path)\n",
        "\n",
        "        syn_slices = extract_all_slices(syn_vol, resize_to)\n",
        "        gt_slices = extract_all_slices(gt_vol, resize_to)\n",
        "\n",
        "        # Ensure both have the same number of slices\n",
        "        min_len = min(len(syn_slices), len(gt_slices))\n",
        "        synthetic_slices.append(syn_slices[:min_len])\n",
        "        ground_truth_slices.append(gt_slices[:min_len])\n",
        "\n",
        "    synthetic_slices = np.concatenate(synthetic_slices, axis=0)\n",
        "    ground_truth_slices = np.concatenate(ground_truth_slices, axis=0)\n",
        "\n",
        "    return synthetic_slices, ground_truth_slices\n",
        "\n",
        "# === File Paths ===\n",
        "synthetic_files = [\n",
        "    \"/notebooks/data/ALLT1w/sub-01_ses-1_T1w_defaced_registered.nii.gz\",\n",
        "    \"/notebooks/data/ALLT1w/sub-02_ses-1_T1w_defaced_registered.nii.gz\",\n",
        "    \"/notebooks/data/ALLT1w/sub-03_ses-1_T1w_defaced_registered.nii.gz\",\n",
        "    \"/notebooks/data/ALLT1w/sub-04_ses-1_T1w_defaced_registered.nii.gz\",\n",
        "    \"/notebooks/data/ALLT1w/sub-05_ses-1_T1w_defaced_registered.nii.gz\",\n",
        "    \"/notebooks/data/ALLT1w/sub-06_ses-1_T1w_defaced_registered.nii.gz\",\n",
        "    \"/notebooks/data/ALLT1w/sub-07_ses-1_T1w_defaced_registered.nii.gz\",\n",
        "    \"/notebooks/data/ALLT1w/sub-08_ses-1_T1w_defaced_registered.nii.gz\",\n",
        "    \"/notebooks/data/ALLT1w/sub-09_ses-1_T1w_defaced_registered.nii.gz\"\n",
        "]\n",
        "\n",
        "ground_truth_files = [\n",
        "    \"/notebooks/data/ALLT1w/sub-01_ses-2_T1w_defaced_registered.nii.gz\",\n",
        "    \"/notebooks/data/ALLT1w/sub-02_ses-2_T1w_defaced_registered.nii.gz\",\n",
        "    \"/notebooks/data/ALLT1w/sub-03_ses-2_T1w_defaced_registered.nii.gz\",\n",
        "    \"/notebooks/data/ALLT1w/sub-04_ses-2_T1w_defaced_registered.nii.gz\",\n",
        "    \"/notebooks/data/ALLT1w/sub-05_ses-2_T1w_defaced_registered.nii.gz\",\n",
        "    \"/notebooks/data/ALLT1w/sub-06_ses-2_T1w_defaced_registered.nii.gz\",\n",
        "    \"/notebooks/data/ALLT1w/sub-07_ses-2_T1w_defaced_registered.nii.gz\",\n",
        "    \"/notebooks/data/ALLT1w/sub-08_ses-2_T1w_defaced_registered.nii.gz\",\n",
        "    \"/notebooks/data/ALLT1w/sub-09_ses-2_T1w_defaced_registered.nii.gz\"\n",
        "]\n",
        "\n",
        "# === Load and Extract ===\n",
        "input_slices, target_slices = load_and_extract_slices(\n",
        "    synthetic_files, ground_truth_files, resize_to=(256, 256)\n",
        ")\n",
        "\n",
        "print(\"Input shape:\", input_slices.shape)\n",
        "print(\"Target shape:\", target_slices.shape)\n",
        "\n",
        "# === Split into Train/Test ===\n",
        "train_input, test_input, train_target, test_target = train_test_split(\n",
        "    input_slices, target_slices, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# === Convert to TensorFlow Datasets ===\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_input, train_target)).batch(1).shuffle(100)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_input, test_target)).batch(1)\n",
        "\n",
        "# === Visual Check ===\n",
        "for input_img, target_img in train_dataset.take(1):\n",
        "    print(\"Input slice shape:\", input_img.shape)\n",
        "    print(\"Target slice shape:\", target_img.shape)"
      ],
      "metadata": {
        "id": "GKyobXUpDVL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_CHANNELS = 1\n",
        "\n",
        "# Define a ResNet block\n",
        "def resnet_block(filters, kernel_size=3, strides=1):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "    def block(x):\n",
        "        skip = x\n",
        "        x = tf.keras.layers.Conv2D(filters, kernel_size, strides=strides, padding='same',\n",
        "                                   kernel_initializer=initializer, use_bias=False)(x)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "        x = tf.keras.layers.ReLU()(x)\n",
        "\n",
        "        x = tf.keras.layers.Conv2D(filters, kernel_size, strides=strides, padding='same',\n",
        "                                   kernel_initializer=initializer, use_bias=False)(x)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "        x = tf.keras.layers.Add()([x, skip])  # Residual connection\n",
        "        x = tf.keras.layers.ReLU()(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    return block\n",
        "\n",
        "# Increase model depth with additional downsampling layers\n",
        "def downsample(filters, size, apply_batchnorm=True):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    result = tf.keras.Sequential()\n",
        "    result.add(tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
        "                                      kernel_initializer=initializer, use_bias=False))\n",
        "    if apply_batchnorm:\n",
        "        result.add(tf.keras.layers.BatchNormalization())\n",
        "    result.add(tf.keras.layers.LeakyReLU())\n",
        "    return result\n",
        "\n",
        "def upsample(filters, size, apply_dropout=False):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    result = tf.keras.Sequential()\n",
        "    result.add(tf.keras.layers.Conv2DTranspose(filters, size, strides=2, padding='same',\n",
        "                                               kernel_initializer=initializer, use_bias=False))\n",
        "    result.add(tf.keras.layers.BatchNormalization())\n",
        "    if apply_dropout:\n",
        "        result.add(tf.keras.layers.Dropout(0.5))\n",
        "    result.add(tf.keras.layers.ReLU())\n",
        "    return result\n",
        "\n",
        "# Updated Generator with deeper layers and ResNet blocks\n",
        "def Generator():\n",
        "    inputs = tf.keras.layers.Input(shape=[256, 256, 1])\n",
        "\n",
        "    # Downsampling with added depth\n",
        "    down_stack = [\n",
        "        downsample(64, 4, apply_batchnorm=False),  # (256 -> 128)\n",
        "        downsample(128, 4),  # (128 -> 64)\n",
        "        downsample(256, 4),  # (64 -> 32)\n",
        "        downsample(512, 4),  # (32 -> 16)\n",
        "        downsample(512, 4),  # (16 -> 8)\n",
        "        downsample(512, 4),  # (8 -> 4)\n",
        "        downsample(512, 4),  # (4 -> 2)\n",
        "        downsample(512, 4),  # (2 -> 1)\n",
        "    ]\n",
        "\n",
        "    # Adding ResNet blocks for deeper feature learning\n",
        "    resnet_blocks = [\n",
        "        resnet_block(512),\n",
        "        resnet_block(512),\n",
        "        resnet_block(512),\n",
        "    ]\n",
        "\n",
        "    up_stack = [\n",
        "        upsample(512, 4, apply_dropout=True),  # (1 -> 2)\n",
        "        upsample(512, 4, apply_dropout=True),  # (2 -> 4)\n",
        "        upsample(512, 4, apply_dropout=True),  # (4 -> 8)\n",
        "        upsample(512, 4),  # (8 -> 16)\n",
        "        upsample(256, 4),  # (16 -> 32)\n",
        "        upsample(128, 4),  # (32 -> 64)\n",
        "        upsample(64, 4),   # (64 -> 128)\n",
        "    ]\n",
        "\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4, strides=2, padding='same',\n",
        "                                           kernel_initializer=initializer, activation='tanh')\n",
        "    x = inputs\n",
        "\n",
        "    # Apply downsampling layers\n",
        "    skips = []\n",
        "    for down in down_stack:\n",
        "        x = down(x)\n",
        "        skips.append(x)\n",
        "\n",
        "    # Apply ResNet blocks\n",
        "    for res_block in resnet_blocks:\n",
        "        x = res_block(x)\n",
        "\n",
        "    skips = reversed(skips[:-1])\n",
        "\n",
        "    # Apply upsampling with skip connections\n",
        "    for up, skip in zip(up_stack, skips):\n",
        "        x = up(x)\n",
        "        x = tf.keras.layers.Concatenate()([x, skip])\n",
        "\n",
        "    x = last(x)\n",
        "\n",
        "    return tf.keras.Model(inputs=inputs, outputs=x)\n",
        "\n",
        "def Discriminator():\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    inp = tf.keras.layers.Input(shape=[256, 256, 1], name='input_image')\n",
        "    tar = tf.keras.layers.Input(shape=[256, 256, 1], name='target_image')\n",
        "    x = tf.keras.layers.concatenate([inp, tar])  # Concatenate the input and target\n",
        "    down1 = downsample(64, 4, False)(x)  # (batch_size, 128, 128, 64)\n",
        "    down2 = downsample(128, 4)(down1)  # (batch_size, 64, 64, 128)\n",
        "    down3 = downsample(256, 4)(down2)  # (batch_size, 32, 32, 256)\n",
        "    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)  # (batch_size, 34, 34, 256)\n",
        "    conv = tf.keras.layers.Conv2D(512, 4, strides=1, kernel_initializer=initializer,\n",
        "                                  use_bias=False)(zero_pad1)  # (batch_size, 31, 31, 512)\n",
        "    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
        "    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
        "    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)  # (batch_size, 33, 33, 512)\n",
        "    last = tf.keras.layers.Conv2D(1, 4, strides=1, kernel_initializer=initializer)(zero_pad2)  # (batch_size, 30, 30, 1)\n",
        "    return tf.keras.Model(inputs=[inp, tar], outputs=last)\n",
        "\n",
        "# Define losses and optimizers\n",
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "LAMBDA = 100\n",
        "\n",
        "def generator_loss(disc_generated_output, gen_output, target):\n",
        "    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
        "    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
        "    return total_gen_loss, gan_loss, l1_loss\n",
        "\n",
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
        "    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
        "    total_disc_loss = real_loss + generated_loss\n",
        "    return total_disc_loss\n",
        "\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n",
        "\n",
        "# Training step function (no print statements here)\n",
        "@tf.function\n",
        "def train_step(input_image, target):\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        gen_output = generator(input_image, training=True)\n",
        "        disc_real_output = discriminator([input_image, target], training=True)\n",
        "        disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
        "\n",
        "        # Calculate losses\n",
        "        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n",
        "        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "    # Apply gradients\n",
        "    generator_gradients = gen_tape.gradient(gen_total_loss, generator.trainable_variables)\n",
        "    discriminator_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))\n",
        "\n",
        "    return gen_total_loss, disc_loss\n",
        "\n",
        "# Training loop\n",
        "def fit(train_ds, steps):\n",
        "    for step, (input_image, target) in train_ds.repeat().take(steps).enumerate():\n",
        "        gen_total_loss, disc_loss = train_step(input_image, target)\n",
        "\n",
        "        # Print progress every 100 steps\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}, Generator Loss: {gen_total_loss:.4f}, Discriminator Loss: {disc_loss:.4f}\")\n",
        "\n",
        "        # Save checkpoint and print progress every 1,000 steps\n",
        "        if (step + 1) % 10000 == 0:\n",
        "            print(f\"Checkpoint saved at step {step + 1}\")\n",
        "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "\n",
        "# Generate some images after training\n",
        "def generate_images(model, test_input, tar):\n",
        "    prediction = model(test_input, training=True)\n",
        "    plt.figure(figsize=(15, 15))\n",
        "    display_list = [test_input[0], tar[0], prediction[0]]\n",
        "    title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
        "    for i in range(3):\n",
        "        plt.subplot(1, 3, i+1)\n",
        "        plt.title(title[i])\n",
        "        plt.imshow(display_list[i] * 0.5 + 0.5)\n",
        "        plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "1m1pEboVDXM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to display a single image with matplotlib\n",
        "def display_image(image, title=None):\n",
        "    plt.figure()\n",
        "    plt.imshow((image + 1) / 2)  # Rescale from [-1, 1] to [0, 1] if normalized\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Display images from train_dataset\n",
        "def display_samples_from_dataset(train_dataset, num_samples=20):\n",
        "    for i, (input_image, target_image) in enumerate(train_dataset.take(num_samples)):\n",
        "        print(f\"Displaying sample {i + 1}\")\n",
        "        # Display input (T1w) and target (T2w) images side by side\n",
        "        plt.figure(figsize=(10, 5))\n",
        "\n",
        "        # Rescale the images back to [0, 1] if they were normalized to [-1, 1]\n",
        "        input_image = (input_image[0] + 1) / 2\n",
        "        target_image = (target_image[0] + 1) / 2\n",
        "\n",
        "        # Display input image\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.imshow(input_image)\n",
        "        plt.title(\"Input Image (T1w)\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Display target image\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.imshow(target_image)\n",
        "        plt.title(\"Target Image (T2w)\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "# Display 5 random samples from the dataset\n",
        "display_samples_from_dataset(train_dataset, num_samples=10)"
      ],
      "metadata": {
        "id": "vto1Ft8XDZal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "fit(train_dataset, steps=100000)"
      ],
      "metadata": {
        "id": "IWSy-dnUDbuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle the dataset and select 5 random samples\n",
        "shuffled_dataset = test_dataset.shuffle(buffer_size=len(test_dataset))\n",
        "\n",
        "# Take 5 random samples from the shuffled dataset\n",
        "for i, (inp, tar) in enumerate(shuffled_dataset.take(5)):\n",
        "    generate_images(generator, inp, tar)"
      ],
      "metadata": {
        "id": "OOZ88HcFDd57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nibabel as nib\n",
        "import tensorflow as tf\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "def compute_metrics_all_axes(gt_volume, pred_volume):\n",
        "    \"\"\"\n",
        "    Compute PSNR, SSIM, and NMSE along all 3 slice orientations.\n",
        "    \"\"\"\n",
        "    assert gt_volume.shape == pred_volume.shape, \"Volumes must have the same shape\"\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for axis in range(3):  # 0 = sagittal, 1 = coronal, 2 = axial\n",
        "        psnr_list, ssim_list, nmse_list = [], [], []\n",
        "\n",
        "        num_slices = gt_volume.shape[axis]\n",
        "\n",
        "        for i in range(num_slices):\n",
        "            if axis == 0:\n",
        "                gt_slice = gt_volume[i, :, :]\n",
        "                pred_slice = pred_volume[i, :, :]\n",
        "            elif axis == 1:\n",
        "                gt_slice = gt_volume[:, i, :]\n",
        "                pred_slice = pred_volume[:, i, :]\n",
        "            else:\n",
        "                gt_slice = gt_volume[:, :, i]\n",
        "                pred_slice = pred_volume[:, :, i]\n",
        "\n",
        "            if np.max(gt_slice) == 0:\n",
        "                continue\n",
        "\n",
        "            mse = np.mean((gt_slice - pred_slice) ** 2)\n",
        "            norm_factor = np.mean(gt_slice ** 2)\n",
        "            nmse_val = mse / norm_factor if norm_factor > 0 else 0\n",
        "            psnr_val = psnr(gt_slice, pred_slice, data_range=4095)\n",
        "            ssim_val = ssim(gt_slice, pred_slice, data_range=4095, gaussian_weights=True)\n",
        "\n",
        "            psnr_list.append(psnr_val)\n",
        "            ssim_list.append(ssim_val)\n",
        "            nmse_list.append(nmse_val)\n",
        "\n",
        "        results[f\"axis_{axis}\"] = {\n",
        "            \"psnr\": np.mean(psnr_list),\n",
        "            \"ssim\": np.mean(ssim_list),\n",
        "            \"nmse\": np.mean(nmse_list)\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "def process_nifti_with_pix2pix(input_nifti_path, ground_truth_nifti_path, output_nifti_path, generator):\n",
        "    \"\"\"\n",
        "    Process a NIfTI volume using a Pix2Pix generator slice-by-slice (axial).\n",
        "    \"\"\"\n",
        "    nifti_img = nib.load(input_nifti_path)\n",
        "    volume = nifti_img.get_fdata()\n",
        "    volume = (volume / 4095.0).astype(np.float32)\n",
        "\n",
        "    gt_nifti = nib.load(ground_truth_nifti_path)\n",
        "    gt_volume = gt_nifti.get_fdata()\n",
        "\n",
        "    reconstructed = np.zeros_like(volume)\n",
        "\n",
        "    for i in range(volume.shape[2]):  # Axial slices\n",
        "        slice_2d = volume[:, :, i]\n",
        "\n",
        "        if np.max(slice_2d) == 0:\n",
        "            continue\n",
        "\n",
        "        original_shape = slice_2d.shape\n",
        "        slice_norm = tf.convert_to_tensor(slice_2d[..., np.newaxis])  # (H, W, 1)\n",
        "        slice_resized = tf.image.resize(slice_norm, (256, 256))\n",
        "        slice_input = tf.expand_dims(slice_resized, axis=0)  # (1, 256, 256, 1)\n",
        "\n",
        "        # Predict (no rgb_to_grayscale needed)\n",
        "        refined_slice = generator(slice_input, training=True)[0]  # (256, 256, 1)\n",
        "        refined_slice = tf.clip_by_value(refined_slice, 0.0, 1.0)\n",
        "\n",
        "        # Resize back to original shape\n",
        "        refined_slice_resized = tf.image.resize(refined_slice, original_shape)\n",
        "        reconstructed[:, :, i] = refined_slice_resized[..., 0].numpy() * 4095  # Denormalize\n",
        "\n",
        "    # Compute metrics\n",
        "    metrics = compute_metrics_all_axes(gt_volume, reconstructed)\n",
        "    for axis, vals in metrics.items():\n",
        "        print(f\"{axis.upper()} — PSNR: {vals['psnr']:.2f} dB | SSIM: {vals['ssim']:.4f} | NMSE: {vals['nmse']:.6f}\")\n",
        "\n",
        "\n",
        "    # Save reconstructed volume as NIfTI\n",
        "    out_nifti = nib.Nifti1Image(reconstructed, nifti_img.affine)\n",
        "    nib.save(out_nifti, output_nifti_path)\n",
        "    print(f\"Saved Pix2Pix output: {output_nifti_path}\")\n",
        "\n",
        "# Example usage\n",
        "input_nifti_path = \"/notebooks/data/ALLT1w/sub-10_ses-1_T1w_defaced_registered.nii.gz\"\n",
        "ground_truth_nifti_path = \"/notebooks/data/ALLT1w/sub-10_ses-2_T1w_defaced_registered.nii.gz\"\n",
        "output_nifti_path = \"pix2pix_output_sub10.nii.gz\"\n",
        "\n",
        "process_nifti_with_pix2pix(input_nifti_path, ground_truth_nifti_path, output_nifti_path, generator)"
      ],
      "metadata": {
        "id": "8d7lOi5fDgY3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}