{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import io\n",
        "import glob\n",
        "import datetime\n",
        "import zipfile\n",
        "import random\n",
        "import requests\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchio as tio\n",
        "from scipy.ndimage import gaussian_filter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
      ],
      "metadata": {
        "id": "0UhR98hKGehv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2be3x0zD2uu"
      },
      "outputs": [],
      "source": [
        "def load_nifti(file_path):\n",
        "    \"\"\"\n",
        "    Load a NIfTI file and normalize it to [0,1] using min-max normalization.\n",
        "    \"\"\"\n",
        "    img = nib.load(file_path)\n",
        "    data = img.get_fdata()\n",
        "    min_val = np.min(data)\n",
        "    max_val = np.max(data)\n",
        "    data = (data - min_val) / (max_val - min_val)  # Min-max normalization\n",
        "    return data.astype(np.float32)\n",
        "\n",
        "def extract_patches_corrected(volume, patch_size=(64, 64, 64), stride=32):\n",
        "    \"\"\"\n",
        "    Extracts 3D patches from the given MRI volume while ensuring correct shape.\n",
        "    \"\"\"\n",
        "    volume = np.expand_dims(volume, axis=0)  # (1, H, W, D) for TorchIO\n",
        "    image = tio.ScalarImage(tensor=volume)\n",
        "    subject = tio.Subject(mri=image)\n",
        "\n",
        "    patch_sampler = tio.GridSampler(\n",
        "        subject,\n",
        "        patch_size=patch_size,\n",
        "        patch_overlap=(stride, stride, stride)\n",
        "    )\n",
        "\n",
        "    patches = np.array([patch['mri'][tio.DATA].numpy() for patch in patch_sampler])\n",
        "\n",
        "    # Fix shape: Remove the first dimension (1, H, W, D) â†’ (H, W, D)\n",
        "    patches = np.squeeze(patches, axis=1)  # Removes the extra dimension\n",
        "\n",
        "    # Ensure final shape is (N, 64, 64, 64, 1) - with channel dimension\n",
        "    patches = np.expand_dims(patches, axis=-1)\n",
        "\n",
        "    return patches\n",
        "\n",
        "def save_patches_to_memmap(file_path, patches, total_patches, patch_size):\n",
        "    \"\"\"\n",
        "    Saves extracted patches to a memory-mapped file and appends patches.\n",
        "    \"\"\"\n",
        "    num_patches = patches.shape[0]\n",
        "    shape = (total_patches + num_patches,) + patch_size  # New shape with total patches\n",
        "\n",
        "    # Open the existing memmap file or create a new one if it doesn't exist\n",
        "    if not os.path.exists(file_path):\n",
        "        # Create a new memmap file with the full expected size\n",
        "        memmap_array = np.memmap(file_path, dtype=np.float32, mode='w+', shape=shape)\n",
        "    else:\n",
        "        # Open the existing file for appending\n",
        "        memmap_array = np.memmap(file_path, dtype=np.float32, mode='r+', shape=shape)\n",
        "\n",
        "    # Remove the channel dimension when saving\n",
        "    memmap_array[total_patches:total_patches + num_patches] = patches.squeeze(axis=-1)  # Remove the last dimension (64,64,64)\n",
        "    del memmap_array  # Ensure data is saved to disk\n",
        "\n",
        "    return total_patches + num_patches  # Return the updated number of patches\n",
        "\n",
        "def load_and_extract_patches_memmap(synthetic_files, ground_truth_files, unpaired_7T_files,\n",
        "                                    patch_size=(64, 64, 64), stride=32, save_dir=\"/notebooks/memmap\"):\n",
        "    \"\"\"\n",
        "    Loads, extracts patches, and saves them in memory-mapped format for efficient loading.\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    synthetic_memmap_path = os.path.join(save_dir, \"synthetic_patches.dat\")\n",
        "    ground_truth_memmap_path = os.path.join(save_dir, \"ground_truth_patches.dat\")\n",
        "    unpaired_7T_memmap_path = os.path.join(save_dir, \"unpaired_7T_patches.dat\")\n",
        "\n",
        "    # Expected number of patches\n",
        "    synthetic_num_patches = 5103\n",
        "    ground_truth_num_patches = 5103\n",
        "    unpaired_7T_num_patches = 11340\n",
        "\n",
        "    total_synthetic_patches = 0\n",
        "    total_ground_truth_patches = 0\n",
        "    total_unpaired_7T_patches = 0\n",
        "\n",
        "    # Initialize memmap files with expected sizes\n",
        "    for memmap_path, num_patches in zip([synthetic_memmap_path, ground_truth_memmap_path, unpaired_7T_memmap_path],\n",
        "                                        [synthetic_num_patches, ground_truth_num_patches, unpaired_7T_num_patches]):\n",
        "        if not os.path.exists(memmap_path):\n",
        "            # Create an empty memmap file with the expected number of patches\n",
        "            memmap_array = np.memmap(memmap_path, dtype=np.float32, mode='w+', shape=(num_patches,) + patch_size)\n",
        "            del memmap_array  # Close the file after creation\n",
        "\n",
        "    # Process synthetic and ground truth pairs\n",
        "    for synthetic_path, ground_truth_path in zip(synthetic_files, ground_truth_files):\n",
        "        print(f\"Processing: {synthetic_path} and {ground_truth_path}\")\n",
        "\n",
        "        synthetic_volume = load_nifti(synthetic_path)\n",
        "        ground_truth_volume = load_nifti(ground_truth_path)\n",
        "\n",
        "        synthetic_patches = extract_patches_corrected(synthetic_volume, patch_size, stride)\n",
        "        ground_truth_patches = extract_patches_corrected(ground_truth_volume, patch_size, stride)\n",
        "\n",
        "        # Save to memory-mapped files and count patches\n",
        "        total_synthetic_patches = save_patches_to_memmap(synthetic_memmap_path, synthetic_patches, total_synthetic_patches, patch_size)\n",
        "        total_ground_truth_patches = save_patches_to_memmap(ground_truth_memmap_path, ground_truth_patches, total_ground_truth_patches, patch_size)\n",
        "\n",
        "    # Process unpaired 7T images\n",
        "    for unpaired_7T_path in unpaired_7T_files:\n",
        "        print(f\"Processing unpaired 7T: {unpaired_7T_path}\")\n",
        "\n",
        "        unpaired_7T_volume = load_nifti(unpaired_7T_path)\n",
        "        unpaired_7T_patches = extract_patches_corrected(unpaired_7T_volume, patch_size, stride)\n",
        "\n",
        "        # Save to memory-mapped file and count patches\n",
        "        total_unpaired_7T_patches = save_patches_to_memmap(unpaired_7T_memmap_path, unpaired_7T_patches, total_unpaired_7T_patches, patch_size)\n",
        "\n",
        "    # Print total number of patches saved to each memmap file\n",
        "    print(f\"Total synthetic patches saved: {total_synthetic_patches}\")\n",
        "    print(f\"Total ground truth patches saved: {total_ground_truth_patches}\")\n",
        "    print(f\"Total unpaired 7T patches saved: {total_unpaired_7T_patches}\")\n",
        "\n",
        "    return synthetic_memmap_path, ground_truth_memmap_path, unpaired_7T_memmap_path\n",
        "\n",
        "# File paths\n",
        "synthetic_files = [\n",
        "    \"/notebooks/data/ALLT1w/sub-01_ses-1_T1w_defaced_registered.nii.gz\",\n",
        "    \"/notebooks/data/ALLT1w/sub-03_ses-1_T1w_defaced_registered.nii.gz\",\n",
        "    \"/notebooks/data/ALLT1w/sub-04_ses-1_T1w_defaced_registered.nii.gz\",\n",
        "    \"/notebooks/data/ALLT1w/sub-05_ses-1_T1w_defaced_registered.nii.gz\",\n",
        "    \"/notebooks/data/ALLT1w/sub-06_ses-1_T1w_defaced_registered.nii.gz\",\n",
        "    \"/notebooks/data/ALLT1w/sub-07_ses-1_T1w_defaced_registered.nii.gz\",\n",
        "    \"/notebooks/data/ALLT1w/sub-08_ses-1_T1w_defaced_registered.nii.gz\",\n",
        "    \"/notebooks/data/ALLT1w/sub-09_ses-1_T1w_defaced_registered.nii.gz\",\n",
        "    \"/notebooks/data/ALLT1w/sub-10_ses-1_T1w_defaced_registered.nii.gz\"\n",
        "]\n",
        "\n",
        "ground_truth_files = [\n",
        "    \"/notebooks/data/ALLT1w/sub-01_ses-2_T1w_defaced_registered.nii.gz\",\n",
        "    \"/notebooks/data/ALLT1w/sub-03_ses-2_T1w_defaced_registered.nii.gz\",\n",
        "    \"/notebooks/data/ALLT1w/sub-04_ses-2_T1w_defaced_registered.nii.gz\",\n",
        "    \"/notebooks/data/ALLT1w/sub-05_ses-2_T1w_defaced_registered.nii.gz\",\n",
        "    \"/notebooks/data/ALLT1w/sub-06_ses-2_T1w_defaced_registered.nii.gz\",\n",
        "    \"/notebooks/data/ALLT1w/sub-07_ses-2_T1w_defaced_registered.nii.gz\",\n",
        "    \"/notebooks/data/ALLT1w/sub-08_ses-2_T1w_defaced_registered.nii.gz\",\n",
        "    \"/notebooks/data/ALLT1w/sub-09_ses-2_T1w_defaced_registered.nii.gz\",\n",
        "    \"/notebooks/data/ALLT1w/sub-10_ses-2_T1w_defaced_registered.nii.gz\"\n",
        "]\n",
        "\n",
        "folder_path = \"/notebooks/pre_training_full\"\n",
        "unpaired_7T_files = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, file))]\n",
        "\n",
        "# Extract patches and save to memmap\n",
        "patch_size = (64, 64, 64)\n",
        "stride = 32\n",
        "\n",
        "synthetic_memmap, ground_truth_memmap, unpaired_7T_memmap = load_and_extract_patches_memmap(\n",
        "    synthetic_files, ground_truth_files, unpaired_7T_files, patch_size, stride\n",
        ")\n",
        "\n",
        "# Print memmap file paths\n",
        "print(f\"Memmap synthetic patches saved at: {synthetic_memmap}\")\n",
        "print(f\"Memmap ground truth patches saved at: {ground_truth_memmap}\")\n",
        "print(f\"Memmap unpaired 7T patches saved at: {unpaired_7T_memmap}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of patches expected\n",
        "synthetic_num_patches = 5103\n",
        "ground_truth_num_patches = 5103\n",
        "unpaired_7T_num_patches = 5670\n",
        "\n",
        "# Define patch size\n",
        "patch_size = (64, 64, 64)\n",
        "\n",
        "# Memory-mapped paths\n",
        "synthetic_memmap_path = \"/notebooks/memmap/synthetic_patches.dat\"\n",
        "ground_truth_memmap_path = \"/notebooks/memmap/ground_truth_patches.dat\"\n",
        "unpaired_7T_memmap_path = \"/notebooks/memmap/unpaired_7T_patches.dat\"\n",
        "\n",
        "# Load memmap arrays\n",
        "synthetic_patches_memmap = np.memmap(synthetic_memmap_path, dtype=np.float32, mode='r', shape=(synthetic_num_patches,) + patch_size)\n",
        "ground_truth_patches_memmap = np.memmap(ground_truth_memmap_path, dtype=np.float32, mode='r', shape=(ground_truth_num_patches,) + patch_size)\n",
        "unpaired_7T_patches_memmap = np.memmap(unpaired_7T_memmap_path, dtype=np.float32, mode='r', shape=(unpaired_7T_num_patches,) + patch_size)\n",
        "\n",
        "# Manually split the paired dataset into training and validation sets\n",
        "validation_size = 0.1  # 10% for validation\n",
        "validation_count = int(synthetic_num_patches * validation_size)\n",
        "\n",
        "# Train/Validation split indices\n",
        "train_indices = slice(validation_count, synthetic_num_patches)\n",
        "val_indices = slice(0, validation_count)\n",
        "\n",
        "# Create TensorFlow datasets\n",
        "train_paired = tf.data.Dataset.from_tensor_slices((\n",
        "    synthetic_patches_memmap[train_indices],\n",
        "    ground_truth_patches_memmap[train_indices]\n",
        "))\n",
        "\n",
        "train_unpaired_7T = tf.data.Dataset.from_tensor_slices(\n",
        "    unpaired_7T_patches_memmap[:synthetic_num_patches - validation_count]  # Adjust length to match paired dataset\n",
        ")\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    synthetic_patches_memmap[val_indices],\n",
        "    ground_truth_patches_memmap[val_indices]\n",
        "))\n",
        "\n",
        "# Combine paired and unpaired data into one training dataset\n",
        "train_dataset = tf.data.Dataset.zip((train_paired, train_unpaired_7T))\n",
        "\n",
        "# Shuffle and batch datasets\n",
        "batch_size = 8\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1000).batch(batch_size)\n",
        "val_dataset = val_dataset.batch(batch_size)"
      ],
      "metadata": {
        "id": "Q2vIVYaZHS5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def residual_block(x, filters):\n",
        "    \"\"\" Residual Block to enhance fine details in feature maps. \"\"\"\n",
        "    res = layers.Conv3D(filters, (3, 3, 3), padding='same')(x)\n",
        "    res = layers.PReLU()(res)\n",
        "    res = layers.Conv3D(filters, (3, 3, 3), padding='same')(res)\n",
        "\n",
        "    # Ensure `x` has the same number of channels as `res`\n",
        "    if x.shape[-1] != filters:\n",
        "        x = layers.Conv3D(filters, (1, 1, 1), padding='same', activation='linear')(x)\n",
        "\n",
        "    return layers.Add()([x, res])  # Residual connection to refine features\n",
        "\n",
        "def multi_scale_fusion(x, filters):\n",
        "    \"\"\" Multi-scale feature extraction to retain details at different receptive fields. \"\"\"\n",
        "    s1 = layers.Conv3D(filters, (1, 1, 1), padding='same')(x)\n",
        "    s3 = layers.Conv3D(filters, (3, 3, 3), padding='same')(x)\n",
        "    s5 = layers.Conv3D(filters, (5, 5, 5), padding='same')(x)\n",
        "    return layers.concatenate([s1, s3, s5], axis=-1)\n",
        "\n",
        "def build_optimized_unet_autoencoder(input_shape=(64, 64, 64, 1)):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Encoder\n",
        "    c1 = layers.Conv3D(32, (5, 5, 5), padding='same')(inputs)\n",
        "    c1 = layers.PReLU()(c1)\n",
        "    c1 = layers.Conv3D(32, (3, 3, 3), padding='same')(c1)\n",
        "    c1 = layers.PReLU()(c1)\n",
        "    c1 = layers.LayerNormalization()(c1)\n",
        "    p1 = layers.MaxPooling3D((2, 2, 2))(c1)  # 32x32x32\n",
        "\n",
        "    c2 = layers.Conv3D(64, (3, 3, 3), padding='same')(p1)\n",
        "    c2 = layers.PReLU()(c2)\n",
        "    c2 = layers.Conv3D(64, (3, 3, 3), padding='same')(c2)\n",
        "    c2 = layers.PReLU()(c2)\n",
        "    c2 = layers.LayerNormalization()(c2)\n",
        "    p2 = layers.MaxPooling3D((2, 2, 2))(c2)  # 16x16x16\n",
        "\n",
        "    c3 = layers.Conv3D(128, (3, 3, 3), padding='same')(p2)\n",
        "    c3 = layers.PReLU()(c3)\n",
        "    c3 = layers.Conv3D(128, (3, 3, 3), padding='same')(c3)\n",
        "    c3 = layers.PReLU()(c3)\n",
        "    c3 = layers.LayerNormalization()(c3)\n",
        "    p3 = layers.MaxPooling3D((2, 2, 2))(c3)  # 8x8x8\n",
        "\n",
        "    c4 = layers.Conv3D(256, (3, 3, 3), padding='same')(p3)\n",
        "    c4 = layers.PReLU()(c4)\n",
        "    c4 = layers.Conv3D(256, (3, 3, 3), padding='same')(c4)\n",
        "    c4 = layers.PReLU()(c4)\n",
        "    c4 = layers.LayerNormalization()(c4)\n",
        "\n",
        "    # Decoder with Residual Blocks and Multi-Scale Fusion\n",
        "    u3 = layers.Conv3DTranspose(128, (3, 3, 3), strides=(2, 2, 2), padding='same')(c4)  # 16x16x16\n",
        "    u3 = layers.concatenate([u3, c3])  # Now both are (16,16,16,128)\n",
        "    u3 = residual_block(u3, 128)\n",
        "    u3 = multi_scale_fusion(u3, 128)\n",
        "    u3 = layers.LayerNormalization()(u3)\n",
        "\n",
        "    u2 = layers.Conv3DTranspose(64, (3, 3, 3), strides=(2, 2, 2), padding='same')(u3)  # 32x32x32\n",
        "    u2 = layers.concatenate([u2, c2])  # Now both are (32,32,32,64)\n",
        "    u2 = residual_block(u2, 64)\n",
        "    u2 = multi_scale_fusion(u2, 64)\n",
        "    u2 = layers.LayerNormalization()(u2)\n",
        "\n",
        "    u1 = layers.Conv3DTranspose(32, (3, 3, 3), strides=(2, 2, 2), padding='same')(u2)  # 64x64x64\n",
        "    u1 = layers.concatenate([u1, c1])  # Now both are (64,64,64,32)\n",
        "    u1 = residual_block(u1, 32)\n",
        "    u1 = multi_scale_fusion(u1, 32)\n",
        "    u1 = layers.LayerNormalization()(u1)\n",
        "\n",
        "    outputs = layers.Conv3D(1, (1, 1, 1), activation='sigmoid', padding='same')(u1)  # Sigmoid for better contrast\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "# Hybrid Loss Function with Higher SSIM Weight\n",
        "def hybrid_loss(y_true, y_pred):\n",
        "    mse_loss = tf.keras.losses.MeanSquaredError()(y_true, y_pred)\n",
        "    ssim_component = 1 - tf.reduce_mean(tf.image.ssim(y_true, y_pred, max_val=1.0))\n",
        "    return mse_loss + 0.7 * ssim_component  # Increased SSIM weight for better structure preservation\n",
        "\n",
        "\n",
        "# Build Model\n",
        "autoencoder = build_optimized_unet_autoencoder()\n",
        "autoencoder.compile(optimizer=Adam(learning_rate=2e-5), loss=hybrid_loss, metrics=['mse'])\n",
        "\n",
        "# Summary\n",
        "autoencoder.summary()"
      ],
      "metadata": {
        "id": "CiNW5T3BELnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to display a slice from a 3D patch\n",
        "def show_slice(image, slice_index=None, title=\"Image Slice\"):\n",
        "    image = image.numpy()\n",
        "\n",
        "    if slice_index is None:\n",
        "        slice_index = image.shape[2] // 2  # middle slice by default\n",
        "\n",
        "    plt.imshow(image[:, :, slice_index], cmap='gray')\n",
        "    plt.title(f\"{title} (slice {slice_index})\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Visualize examples from training dataset\n",
        "for (paired_batch, unpaired_batch) in train_dataset.take(1):\n",
        "    x_3T_batch, y_7T_batch = paired_batch\n",
        "\n",
        "    # Display first sample from paired data (synthetic and ground truth)\n",
        "    show_slice(x_3T_batch[0], title=\"Synthetic (3T) Patch\")\n",
        "    show_slice(y_7T_batch[0], title=\"Ground Truth (7T) Patch\")\n",
        "\n",
        "    # Display first sample from unpaired data\n",
        "    show_slice(unpaired_batch[0], title=\"Unpaired 7T Patch\")\n",
        "\n",
        "# Visualize examples from validation dataset\n",
        "for (x_val, y_val) in val_dataset.take(1):\n",
        "    show_slice(x_val[0], title=\"Validation Synthetic (3T)\")\n",
        "    show_slice(y_val[0], title=\"Validation Ground Truth (7T)\")"
      ],
      "metadata": {
        "id": "dc9ZEkUaHoEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hybrid loss function (Mean Squared Error + SSIM for paired data)\n",
        "def hybrid_loss(y_true, y_pred):\n",
        "    mse_loss = tf.keras.losses.MeanSquaredError()(y_true, y_pred)\n",
        "    ssim_loss = 1 - tf.reduce_mean(tf.image.ssim(y_true, y_pred, max_val=1.0))\n",
        "    return mse_loss + 0.7 * ssim_loss  # Adjust SSIM weight for better structure preservation\n",
        "\n",
        "# Consistency loss (used for unpaired 7T data)\n",
        "def consistency_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Consistency loss for unpaired 7T MRI data (L1 + SSIM).\n",
        "    \"\"\"\n",
        "    l1_loss = tf.reduce_mean(tf.abs(y_true - y_pred))\n",
        "    ssim_loss = 1 - tf.reduce_mean(tf.image.ssim(y_true, y_pred, max_val=1.0))\n",
        "    return l1_loss + 0.1 * ssim_loss  # Adjust SSIM weight if needed\n",
        "\n",
        "# Build the model\n",
        "autoencoder = build_optimized_unet_autoencoder()  # Assuming this is defined earlier\n",
        "\n",
        "# Compile the model (using hybrid loss for supervised learning)\n",
        "autoencoder.compile(optimizer=Adam(learning_rate=2e-5), loss=hybrid_loss, metrics=['mse'])\n",
        "\n",
        "class SemiSupervisedAutoencoder(tf.keras.Model):\n",
        "    def __init__(self, autoencoder, lambda_consistency=0.5):\n",
        "        super().__init__()\n",
        "        self.autoencoder = autoencoder\n",
        "        self.lambda_consistency = lambda_consistency\n",
        "\n",
        "        # Metrics\n",
        "        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
        "        self.supervised_loss_tracker = tf.keras.metrics.Mean(name=\"supervised_loss\")\n",
        "        self.consistency_loss_tracker = tf.keras.metrics.Mean(name=\"consistency_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.supervised_loss_tracker,\n",
        "            self.consistency_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        ((x_3T, y_7T), unpaired_7T) = data\n",
        "\n",
        "        x_3T = tf.expand_dims(x_3T, axis=-1)\n",
        "        y_7T = tf.expand_dims(y_7T, axis=-1)\n",
        "        unpaired_7T = tf.expand_dims(unpaired_7T, axis=-1)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self.autoencoder(x_3T, training=True)\n",
        "            supervised_loss = hybrid_loss(y_7T, y_pred)\n",
        "\n",
        "            unpaired_pred = self.autoencoder(unpaired_7T, training=True)\n",
        "            consistency_loss_value = consistency_loss(unpaired_7T, unpaired_pred)\n",
        "\n",
        "            total_loss = supervised_loss + self.lambda_consistency * consistency_loss_value\n",
        "\n",
        "        # Compute gradients explicitly\n",
        "        grads = tape.gradient(total_loss, self.autoencoder.trainable_weights)\n",
        "\n",
        "        # Verify gradients are not None or zero\n",
        "        grad_norm = tf.linalg.global_norm(grads)\n",
        "\n",
        "        # Apply gradients\n",
        "        self.optimizer.apply_gradients(zip(grads, self.autoencoder.trainable_weights))\n",
        "\n",
        "        # Update trackers\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.supervised_loss_tracker.update_state(supervised_loss)\n",
        "        self.consistency_loss_tracker.update_state(consistency_loss_value)\n",
        "\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"supervised_loss\": self.supervised_loss_tracker.result(),\n",
        "            \"consistency_loss\": self.consistency_loss_tracker.result(),\n",
        "        }\n",
        "\n",
        "    def test_step(self, data):\n",
        "        (x_3T, y_7T) = data\n",
        "        x_3T = tf.expand_dims(x_3T, axis=-1)\n",
        "        y_7T = tf.expand_dims(y_7T, axis=-1)\n",
        "        y_pred = self.autoencoder(x_3T, training=False)\n",
        "        supervised_loss = hybrid_loss(y_7T, y_pred)\n",
        "\n",
        "        self.supervised_loss_tracker.update_state(supervised_loss)\n",
        "        return {\n",
        "            \"supervised_loss\": self.supervised_loss_tracker.result()\n",
        "        }\n",
        "\n",
        "# Define Callbacks\n",
        "checkpoint = ModelCheckpoint(\"best_7T.keras\", save_best_only=True, monitor=\"val_supervised_loss\", verbose=1)\n",
        "early_stopping = EarlyStopping(monitor=\"val_supervised_loss\", patience=10, restore_best_weights=True)\n",
        "\n",
        "# Initialize the custom semi-supervised autoencoder model\n",
        "semi_supervised_autoencoder = SemiSupervisedAutoencoder(autoencoder, lambda_consistency=0.5)\n",
        "\n",
        "# Compile the model\n",
        "semi_supervised_autoencoder.compile(optimizer=Adam(learning_rate=2e-5), loss=hybrid_loss, metrics=['mse'])\n",
        "\n",
        "# Train the model using semi-supervised learning\n",
        "semi_supervised_autoencoder.fit(\n",
        "    train_dataset,\n",
        "    epochs=30,\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=[checkpoint, early_stopping]\n",
        ")"
      ],
      "metadata": {
        "id": "QoZ2bnFbEOth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict and visualize one patch\n",
        "testt = tf.expand_dims(synthetic_patches_memmap, axis=-1)\n",
        "predicted_patches = semi_supervised_autoencoder.predict(testt[:150])\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(testt[100, :, :, :, 0][32], cmap='gray')\n",
        "plt.title(\"Synthetic Patch (Slice 32)\")\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(predicted_patches[100, :, :, :, 0][32], cmap='gray')\n",
        "plt.title(\"Predicted Patch (Slice 32)\")"
      ],
      "metadata": {
        "id": "MmLx9gNWFqag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_psnr_ssim_nmse(ground_truth_volume, refined_volume):\n",
        "    \"\"\"\n",
        "    Computes PSNR, SSIM, and NMSE for 3D MRI volumes slice-by-slice and returns their mean.\n",
        "    \"\"\"\n",
        "    # Ensure both volumes have the same shape\n",
        "    assert ground_truth_volume.shape == refined_volume.shape, \"Volume shapes do not match!\"\n",
        "\n",
        "    for j in range(3):\n",
        "\n",
        "      psnr_values = []\n",
        "      ssim_values = []\n",
        "      nmse_values = []\n",
        "\n",
        "      num_slices = ground_truth_volume.shape[j]  # Assuming slices along the Z-axis\n",
        "\n",
        "      for i in range(num_slices):\n",
        "\n",
        "          if j == 0:\n",
        "            gt_slice = ground_truth_volume[i, :, :]\n",
        "            pred_slice = refined_volume[i, :, :]\n",
        "\n",
        "          if j == 1:\n",
        "            gt_slice = ground_truth_volume[:, i, :]\n",
        "            pred_slice = refined_volume[:, i, :]\n",
        "\n",
        "          if j == 2:\n",
        "            gt_slice = ground_truth_volume[:, :, i]\n",
        "            pred_slice = refined_volume[:, :, i]\n",
        "\n",
        "          # Compute MSE\n",
        "          mse = np.mean((gt_slice - pred_slice) ** 2)\n",
        "\n",
        "          # Compute PSNR (handle zero-MSE case)\n",
        "          psnr_value = psnr(gt_slice, pred_slice, data_range=4095)\n",
        "\n",
        "          # Compute SSIM\n",
        "          ssim_value = ssim(gt_slice, pred_slice, data_range=4095, gaussian_weights=True)\n",
        "\n",
        "          # Compute NMSE (Normalized Mean Squared Error)\n",
        "          norm_factor = np.mean(gt_slice ** 2)  # Normalize by the mean squared value of the ground truth\n",
        "          nmse_value = mse / norm_factor if norm_factor > 0 else 0  # Avoid division by zero\n",
        "\n",
        "          psnr_values.append(psnr_value)\n",
        "          ssim_values.append(ssim_value)\n",
        "          nmse_values.append(nmse_value)\n",
        "\n",
        "      # Compute mean values across all slices, ignoring infinite PSNR values\n",
        "      mean_psnr = np.mean([p for p in psnr_values if np.isfinite(p)]) if psnr_values else 0\n",
        "      mean_ssim = np.mean(ssim_values) if ssim_values else 0\n",
        "      mean_nmse = np.mean(nmse_values) if nmse_values else 0\n",
        "\n",
        "      print(f'Axis {j}: Mean PSNR: {mean_psnr:.2f} dB, Mean SSIM: {mean_ssim:.4f}, Mean NMSE: {mean_nmse:.6f}')\n",
        "\n",
        "    return mean_psnr, mean_ssim, mean_nmse"
      ],
      "metadata": {
        "id": "xgoIhxZ4Fzm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_patches(volume, patch_size, stride):\n",
        "    \"\"\"\n",
        "    Extract 3D patches from a volume, ensuring full coverage with padding at edges.\n",
        "    \"\"\"\n",
        "    h, w, d = volume.shape\n",
        "    ps_h, ps_w, ps_d = patch_size\n",
        "\n",
        "    patches = []\n",
        "    positions = []\n",
        "\n",
        "    for i in range(0, h, stride):\n",
        "        for j in range(0, w, stride):\n",
        "            for k in range(0, d, stride):\n",
        "                # Ensure patches at edges do not exceed image size\n",
        "                i_end = min(i + ps_h, h)\n",
        "                j_end = min(j + ps_w, w)\n",
        "                k_end = min(k + ps_d, d)\n",
        "\n",
        "                # Extract patch\n",
        "                patch = volume[i:i_end, j:j_end, k:k_end]\n",
        "\n",
        "                # If patch is smaller due to edge effects, pad it\n",
        "                pad_h = ps_h - (i_end - i)\n",
        "                pad_w = ps_w - (j_end - j)\n",
        "                pad_d = ps_d - (k_end - k)\n",
        "\n",
        "                patch = np.pad(patch, ((0, pad_h), (0, pad_w), (0, pad_d)), mode='reflect')\n",
        "\n",
        "                patches.append(patch)\n",
        "                positions.append((i, j, k))\n",
        "\n",
        "    patches = np.array(patches)\n",
        "    patches = np.expand_dims(patches, axis=-1)  # Add channel dimension\n",
        "    return patches, positions\n",
        "\n",
        "def gaussian_weight_map(shape, sigma=0.5):\n",
        "    \"\"\"Generate a Gaussian weighting map for smooth patch blending.\"\"\"\n",
        "    mask = np.ones(shape, dtype=np.float32)\n",
        "    weights = gaussian_filter(mask, sigma=sigma)\n",
        "    return weights / np.max(weights)  # Normalize\n",
        "\n",
        "def reconstruct_volume(patches, positions, original_shape, patch_size):\n",
        "    \"\"\"\n",
        "    Reconstruct a 3D volume from overlapping patches using Gaussian blending.\n",
        "    \"\"\"\n",
        "    h, w, d = original_shape\n",
        "    ps_h, ps_w, ps_d = patch_size\n",
        "\n",
        "    reconstructed = np.zeros(original_shape, dtype=np.float32)\n",
        "    counts = np.zeros(original_shape, dtype=np.float32)\n",
        "\n",
        "    weight_map = gaussian_weight_map((ps_h, ps_w, ps_d))  # Generate smooth weight map\n",
        "\n",
        "    for idx, (i, j, k) in enumerate(positions):\n",
        "        i_end = min(i + ps_h, h)\n",
        "        j_end = min(j + ps_w, w)\n",
        "        k_end = min(k + ps_d, d)\n",
        "\n",
        "        patch = patches[idx, ..., 0]  # Extract patch\n",
        "        patch = patch[:i_end-i, :j_end-j, :k_end-k]  # Crop to match original size if needed\n",
        "        weight = weight_map[:i_end-i, :j_end-j, :k_end-k]  # Crop weight map similarly\n",
        "\n",
        "        reconstructed[i:i_end, j:j_end, k:k_end] += patch * weight\n",
        "        counts[i:i_end, j:j_end, k:k_end] += weight\n",
        "\n",
        "    counts[counts == 0] = 1  # Avoid division by zero\n",
        "    return reconstructed / counts  # Normalize final volume\n",
        "\n",
        "def process_nifti_with_autoencoder(input_nifti_path, ground_truth_nifti_path, output_nifti_path, autoencoder, patch_size=(64,64,64), stride=32):\n",
        "    \"\"\"\n",
        "    Processes a single NIfTI file through the autoencoder and saves the reconstructed volume.\n",
        "    \"\"\"\n",
        "    # Load NIfTI file\n",
        "    nifti_img = nib.load(input_nifti_path)\n",
        "    volume = nifti_img.get_fdata()\n",
        "\n",
        "        # Load NIfTI file\n",
        "    nifti_gt = nib.load(ground_truth_nifti_path)\n",
        "    volume_gt = nifti_gt.get_fdata()\n",
        "\n",
        "\n",
        "    # Normalize input to [-1,1]\n",
        "    volume = (volume/4095)\n",
        "\n",
        "    # Extract patches\n",
        "    synthetic_patches, positions = extract_patches(volume, patch_size, stride)\n",
        "\n",
        "    # Predict refined patches\n",
        "    refined_patches = autoencoder.predict(synthetic_patches)\n",
        "\n",
        "    # Reconstruct volume\n",
        "    refined_volume = reconstruct_volume(refined_patches, positions, volume.shape, patch_size)\n",
        "\n",
        "    # Denormalize input\n",
        "    refined_volume = (refined_volume) * 4095\n",
        "\n",
        "    # Example Usage:\n",
        "    mean_psnr, mean_ssim, mean_nmse = compute_psnr_ssim_nmse(volume_gt, refined_volume)\n",
        "\n",
        "    # Save as NIfTI\n",
        "    refined_nifti = nib.Nifti1Image(refined_volume, nifti_img.affine)\n",
        "    nib.save(refined_nifti, output_nifti_path)\n",
        "    print(f\"Saved refined MRI: {output_nifti_path}\")\n",
        "\n",
        "# Example Usage\n",
        "input_nifti_path = \"/notebooks/data/ALLT1w/sub-08_ses-1_T1w_defaced_registered.nii.gz\"\n",
        "ground_truth_nifti_path = \"/notebooks/data/ALLT1w/sub-08_ses-2_T1w_defaced_registered.nii.gz\"\n",
        "output_nifti_path = \"Test_sub08_SAE.nii.gz\"\n",
        "\n",
        "process_nifti_with_autoencoder(input_nifti_path, ground_truth_nifti_path, output_nifti_path, autoencoder, patch_size=(64,64,64), stride=32)"
      ],
      "metadata": {
        "id": "coc--bGuF14V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}